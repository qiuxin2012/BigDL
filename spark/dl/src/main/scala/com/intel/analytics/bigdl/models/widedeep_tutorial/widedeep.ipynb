{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide and Deep Learning on BigDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide and Deep Learning Model, proposed by Google in 2016, is a DNN-Linear mixed model. Wide and deep learning has been used for Google App Store for their app recommendation.\n",
    "\n",
    "In this tutorial, we'll introduce how to use the new SparseTensor API to train a wide linear model and a deep neural network, which is called wide and deep model, on BigDL. Wide and deep model combines the strength of memorization and generalization. It's useful for generic large-scale regression and classification problems with sparse input features(e.g., categorical features with a large number of possible feature values).\n",
    "\n",
    "If you are interested in learning more about how Wide and Deep Learning model works, you can check out:\n",
    "[Google Research Paper](https://arxiv.org/abs/1606.07792) or [Tensorflow tutorials](https://www.tensorflow.org/tutorials/wide_and_deep).\n",
    "\n",
    "You can refer to Tensorflow tutorial to get the detailed description of wide and deep model.\n",
    "\n",
    "On BigDL, there are some points different from Tensorflow:\n",
    "\n",
    "1. Our SparseTensor data structure is different from Tensorflow.\n",
    "2. We use only one Optimizer Adam to optimize both wide part and deep part of the model.\n",
    "3. Our model building is different from Tensorflow.\n",
    "\n",
    "And that's it! Let's go through a simple example.\n",
    "\n",
    "The example is to train a wide and deep model to predict the probability that the individual has an annual income of over 50,000 dollars using the [Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To try the code for this tutorial:\n",
    "\n",
    "1. [Install BigDL](https://bigdl-project.github.io/master/#ScalaUserGuide/install-build-src/) if you haven't already.\n",
    "2. Download [the tutorial code](Code link).\n",
    "3. Execute the tutorial code with the following command to train a wide and deep model.\n",
    "\n",
    "```\n",
    "BigDL_HOME=...\n",
    "BigDL_VERSION=0.3.0-SNAPSHOT\n",
    "PYTHON_API_PATH=${BigDL_HOME}/dist/lib/bigdl-${BigDL_VERSION}-python-api.zip\n",
    "BigDL_JAR_PATH=${BigDL_HOME}/dist/lib/bigdl-${BigDL_VERSION}-jar-with-dependencies.jar\n",
    "PYTHONPATH=${PYTHON_API_PATH}:$PYTHONPATH\n",
    "\n",
    "MASTER=local[24]\n",
    "\n",
    "spark-submit \\\n",
    "    --master ${MASTER} \\\n",
    "    --driver-cores 24 \\\n",
    "    --driver-memory 80g \\\n",
    "    --executor-cores 24  \\\n",
    "    --executor-memory 180g \\\n",
    "    --total-executor-cores 24 \\\n",
    "    --conf spark.rpc.message.maxSize=1024 \\\n",
    "    --properties-file ${BigDL_HOME}/dist/conf/spark-bigdl.conf \\\n",
    "    --jars ${BigDL_JAR_PATH} \\\n",
    "    --conf spark.driver.extraClassPath=${BigDL_JAR_PATH} \\\n",
    "    --conf spark.executor.extraClassPath=${BigDL_JAR_PATH} \\\n",
    "    --class com.intel.analytics.bigdl.models.widedeep_tutorial.Train \\\n",
    "    ${BigDL_JAR_PATH} \\\n",
    "    -f ${BigDL_HOME}/census \\\n",
    "    -b 1200 \\\n",
    "    -e 100 \\\n",
    "    -r 0.001\n",
    "```\n",
    "\n",
    "`census` directory stores your train data and test data.\n",
    "\n",
    "```\n",
    "./census$ tree .\n",
    ".\n",
    "├── train.data\n",
    "├── test.data\n",
    "```\n",
    "\n",
    "* ```--f``` option can be used to set data folder, which contains train and test data.\n",
    "\n",
    "* ```--b``` option can be used to set batch size, the default value is 128.\n",
    "\n",
    "* ```--e``` option can be used to control how to end the training process.\n",
    "\n",
    "* ```--r``` option can be used to control the initial learning rate for training.\n",
    "\n",
    "\n",
    "To verify the accuracy, search \"accuracy\" from log:\n",
    "\n",
    "```\n",
    "INFO  DistriOptimizer$:247 - [Epoch 1 0/32561][Iteration 1][Wall Clock 0.0s] Train 1280 in xx seconds. Throughput is xx records/second.\n",
    "\n",
    "INFO  DistriOptimizer$:629 - Top1Accuracy is Accuracy(correct: 13843, count: 16281, accuracy: 0.8502548983477674)\n",
    "```\n",
    "\n",
    "Read on to find out how this code builds its wide and deep model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary classes and objects\n",
    "\n",
    "Firstly we import the necessary classes and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.intel.analytics.bigdl.dataset.SparseTensorMiniBatch\n",
    "import com.intel.analytics.bigdl.nn.{ClassNLLCriterion, Module}\n",
    "import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "import com.intel.analytics.bigdl.models.widedeep_tutorial.SparseWideDeep\n",
    "import com.intel.analytics.bigdl.optim._\n",
    "import com.intel.analytics.bigdl.tensor.Tensor\n",
    "import com.intel.analytics.bigdl.utils.{Engine, LoggerFilter}\n",
    "import com.intel.analytics.bigdl.dataset.{Sample, TensorSample}\n",
    "import com.intel.analytics.bigdl.tensor.{Storage, Tensor}\n",
    "import com.intel.analytics.bigdl.utils.{File, T}\n",
    "import com.intel.analytics.bigdl.visualization.{TrainSummary, ValidationSummary}\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkContext\n",
    "import scopt.OptionParser\n",
    "import java.nio.file.Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New SparkContext to run the training process\n",
    "\n",
    "Secondly, we new a SparkContext for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val conf = Engine.createSparkConf().setAppName(\"Wide and Deep Learning on Census\").set(\"spark.rpc.message.maxSize\", \"1024\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "val sc = new SparkContext(conf)\n",
    "Engine.init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Base Feature Columns and Columns' Vocabulary List\n",
    "\n",
    "In this part, we define the base categorical and continuous feature columns that we'll use. These base columns will be the building blocks used by both the wide part and the deep part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val AGE = 0\n",
    "val WORKCLASS = 1\n",
    "val FNLWGT = 2\n",
    "val EDUCATION = 3\n",
    "val EDUCATION_NUM = 4\n",
    "val MARITAL_STATUS = 5\n",
    "val OCCUPATION = 6\n",
    "val RELATIONSHIP = 7\n",
    "val RACE = 8\n",
    "val GENDER = 9\n",
    "val CAPITAL_GAIN = 10\n",
    "val CAPITAL_LOSS = 11\n",
    "val HOURS_PER_WEEK = 12\n",
    "val NATIVE_COUNTRY = 13\n",
    "val LABEL = 14\n",
    "\n",
    "val LABEL_COLUMN = \"label\"\n",
    "val CSV_COLUMNS = Array(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "    \"income_bracket\")\n",
    "val CATEGORICAL_COLUMNS = Array(\"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"gender\", \"native_country\")\n",
    "val CONTINUOUS_COLUMNS = Array(\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n",
    "    \"hours_per_week\")\n",
    "val education_vocab = Array(\"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "    \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "    \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "    \"Preschool\", \"12th\") // 16\n",
    "val marital_status_vocab = Array(\"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n",
    "    \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\")\n",
    "val relationship_vocab = Array(\"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n",
    "    \"Other-relative\")  // 6\n",
    "val workclass_vocab = Array(\"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n",
    "    \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\") // 9\n",
    "val gender_vocab = Array(\"Female\", \"Male\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Methods For Feature Columns\n",
    "\n",
    "Now we define some methods for feature columns.\n",
    "`getAgeboundaries(age, start)` accepts age of one person, and returns the age buckets he/she falls in. Age boundaries is [18, 25, 30, 35, 40, 45, 50, 55, 60, 65].\n",
    "`hashbucket(sth, bucketsize, start)` accepts a string, gets its hash code, and then puts it into a bucket of a lots of buckets, whose number is controlled by bucketsize.\n",
    "`categoricalFromVocabList(sth, vocab_list, default, start)` accepts a string and a vocabulary list of feature column, returns the category of the feature value with respect to the feature column of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAgeboundaries(age: String, start: Int = 0): Int = {\n",
    "    if (age == \"?\") 0 + start\n",
    "    else {\n",
    "      val numage = age.toInt\n",
    "      if (numage < 18) 0 else if (numage < 25) 1 else if (numage < 30) 2 else if (numage < 35) 3\n",
    "      else if (numage < 40) 4 else if (numage < 45) 5 else if (numage < 50) 6\n",
    "      else if (numage < 55) 7 else if (numage < 60) 8 else if (numage < 65) 9 else 10\n",
    "    }\n",
    "  }\n",
    "\n",
    "def hashbucket(sth: String, bucketsize: Int = 1000, start: Int = 0): Int = {\n",
    "    (sth.hashCode() % bucketsize + bucketsize) % bucketsize + start\n",
    "  }\n",
    "\n",
    "def categoricalFromVocabList(sth: String,\n",
    "    vocab_list: Array[String], default: Int = 1, start: Int = 0): Int = {\n",
    "    start + (if (vocab_list.contains(sth)) vocab_list.indexOf(sth) else default)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Procedure\n",
    "\n",
    "Next we'll define our preprocessing procedure, which is included by a function `load()`.\n",
    "`load(src, tag)` reads the train data or test data (controlled by tag, which range from \"Train\" to \"Test\"), and return a RDD of TensorSample[Float], which consists of a feature array and a label array. Feature array contains a SparseTensor and a DenseTensor, represents the input of the wide part and the deep part of the model respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/**\n",
    " * Load data of Census dataset.\n",
    " *\n",
    " * @param src RDD of file line\n",
    " * @param tag \"Train\" or \"Test\", represents train data or test data\n",
    " * @return\n",
    " */\n",
    "def load(src: RDD[String], tag: String = \"Train\"): RDD[Sample[Float]] = {\n",
    "\n",
    "    val iter = if (tag == \"Train\") src.filter(s => (s.length > 0)).map(_.trim().split(\",\"))\n",
    "    else src.filter(s => (!s.contains(\"|1x3 Cross validator\") && s.length > 0))\n",
    "      .map(_.trim().split(\",\"))\n",
    "\n",
    "    val storage = Storage[Float](11)\n",
    "    val storageArray = storage.array()\n",
    "    val results = iter.map(line => {\n",
    "      val indices = new Array[Int](11)\n",
    "      val lis = line.map(_.trim()).toSeq\n",
    "      for (k <- 0 until 5) indices(k) = k\n",
    "      indices(5) = hashbucket(lis(OCCUPATION), 1000, start = 0) + 5\n",
    "      indices(6) = hashbucket(lis(NATIVE_COUNTRY), 1000, start = 0) + 1005\n",
    "      indices(7) = 2005\n",
    "      indices(8) = hashbucket(lis(EDUCATION) + lis(OCCUPATION), 1000, start = 0) + 2006 // 2006\n",
    "      indices(9) = hashbucket(\n",
    "        getAgeboundaries(lis(AGE)).toString + lis(EDUCATION) + lis(OCCUPATION), 1000) + 3006 // 2006\n",
    "      indices(10) = hashbucket(lis(NATIVE_COUNTRY) + lis(OCCUPATION), 1000) + 4006 // 4006\n",
    "\n",
    "      // 5006\n",
    "      storageArray(0) = categoricalFromVocabList(lis(GENDER), gender_vocab, default = -1, start = 0)\n",
    "      storageArray(1) = categoricalFromVocabList(\n",
    "        lis(EDUCATION), education_vocab, default = -1, start = 0)\n",
    "      storageArray(2) = categoricalFromVocabList(\n",
    "        lis(MARITAL_STATUS), marital_status_vocab, default = -1, start = 0)\n",
    "      storageArray(3) = categoricalFromVocabList(\n",
    "        lis(RELATIONSHIP), relationship_vocab, default = -1, start = 0)\n",
    "      storageArray(4) = categoricalFromVocabList(\n",
    "        lis(WORKCLASS), workclass_vocab, default = -1, start = 0)\n",
    "\n",
    "      storageArray(5) = 1\n",
    "      storageArray(6) = 1\n",
    "      storageArray(7) = getAgeboundaries(lis(AGE), 0)\n",
    "\n",
    "      for (k <- 8 until 11) storageArray(k) = 1\n",
    "      // wide model input\n",
    "      val sps = Tensor.sparse(Array(indices), storage, Array(5006), 11)\n",
    "      // deep model input\n",
    "      val den = Tensor[Float](40).fill(0)\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(WORKCLASS), workclass_vocab, start = 1), 1\n",
    "      ) // 9\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(EDUCATION), education_vocab, start = 10), 1\n",
    "      ) // 16\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(GENDER), gender_vocab, start = 26), 1\n",
    "      ) // 2\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(RELATIONSHIP), relationship_vocab,\n",
    "          start = 28), 1\n",
    "      ) // 6\n",
    "      // total : 33\n",
    "      den.setValue(34, hashbucket(lis(NATIVE_COUNTRY), 1000, 1).toFloat)\n",
    "      den.setValue(35, hashbucket(lis(OCCUPATION), 1000, 1).toFloat)\n",
    "      den.setValue(36, lis(AGE).toFloat)\n",
    "      den.setValue(37, lis(EDUCATION_NUM).toFloat)\n",
    "      den.setValue(38, lis(CAPITAL_GAIN).toFloat)\n",
    "      den.setValue(39, lis(CAPITAL_LOSS).toFloat)\n",
    "      den.setValue(40, lis(HOURS_PER_WEEK).toFloat)\n",
    "      den.resize(1, 40)\n",
    "      val train_label = if (lis(LABEL).contains(\">50K\")) Tensor[Float](T(2.0f))\n",
    "                        else Tensor[Float](T(1.0f))\n",
    "      train_label.resize(1, 1)\n",
    "\n",
    "      TensorSample[Float](Array(sps, den), Array(train_label))\n",
    "    })\n",
    "    results\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Dataset and the Model\n",
    "\n",
    "Next we define the loaded train dataset and test dataset, as well as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential[acc0f503]{\n",
      "  [input -> (1) -> (2) -> (3) -> (4) -> output]\n",
      "  (1): nn.ParallelTable {\n",
      "\tinput\n",
      "\t  |`-> (1): Identity[861afa45]\n",
      "\t   `-> (2): Sequential[7fca3bfc]{\n",
      "\t         [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> output]\n",
      "\t         (1): Concat[ff0bdcec]{\n",
      "\t           input\n",
      "\t             |`-> (1): Sequential[94b0e990]{\n",
      "\t             |      [input -> (1) -> (2) -> output]\n",
      "\t             |      (1): Narrow[109d8acd](2, 1, 33)\n",
      "\t             |      (2): Reshape[indicator](33)\n",
      "\t             |    }\n",
      "\t             |`-> (2): Sequential[76011249]{\n",
      "\t             |      [input -> (1) -> (2) -> output]\n",
      "\t             |      (1): nn.Select\n",
      "\t             |      (2): LookupTable[embedding_1](1000, 8, 0.0, 1.7976931348623157E308, 2.0)\n",
      "\t             |    }\n",
      "\t             |`-> (3): Sequential[e1a8f3a8]{\n",
      "\t             |      [input -> (1) -> (2) -> output]\n",
      "\t             |      (1): nn.Select\n",
      "\t             |      (2): LookupTable[embedding_2](1000, 8, 0.0, 1.7976931348623157E308, 2.0)\n",
      "\t             |    }\n",
      "\t             |`-> (4): Sequential[c05ca412]{\n",
      "\t                    [input -> (1) -> (2) -> output]\n",
      "\t                    (1): Narrow[52ce2f8e](2, 36, 5)\n",
      "\t                    (2): Reshape[e39b60ce](5)\n",
      "\t                  }\n",
      "\t              ... -> output\n",
      "\t           }\n",
      "\t         (2): Linear[fc_1](54 -> 100)\n",
      "\t         (3): ReLU[ad8ce7c1](0.0, 0.0)\n",
      "\t         (4): Linear[fc_2](100 -> 50)\n",
      "\t         (5): ReLU[8bc71586](0.0, 0.0)\n",
      "\t         (6): ToSparse[5d5d8322]\n",
      "\t       }\n",
      "\t   ... -> output\n",
      "}\n",
      "  (2): SparseJoinTable[f3616b68]\n",
      "  (3): nn.SparseLinear(5056 -> 2)\n",
      "  (4): LogSoftMax[4d43d186]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "val batchSize = 1200\n",
    "val trainData = \"./census/train.data\"\n",
    "val testData = \"./census/test.data\"\n",
    "val trainDataSet = load(sc.textFile(Paths.get(trainData).toString), \"Train\")\n",
    "val validateSet = load(sc.textFile(Paths.get(testData).toString), \"Test\")\n",
    "\n",
    "val model = SparseWideDeep[Float](modelType = \"wide_n_deep\", classNum = 2)\n",
    "println(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Optimizer\n",
    "\n",
    "In this part, we'll define the optimize method and the optimizer, which is one of the key part of model training.\n",
    "We set the Adam optimizer to optimize our wide and deep model, and set learning rate 0.001, as well as learning rate decay 0.0005.\n",
    "Then we set the model we want to train, and set the trainDataset we loaded above, then we set criterion, batch size and miniBatch form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val optimMethod = new Adam[Float](\n",
    "          learningRate = 0.001,\n",
    "          learningRateDecay = 0.0005\n",
    "        )\n",
    "\n",
    "val optimizer = Optimizer(\n",
    "        model = model,\n",
    "        sampleRDD = trainDataSet,\n",
    "        criterion = ClassNLLCriterion[Float](),\n",
    "        batchSize = batchSize,\n",
    "        miniBatch = new SparseTensorMiniBatch[Float](Array(\n",
    "          Tensor.sparse(Array(5006), 1),\n",
    "          Tensor(1, 40)),\n",
    "          Array(Tensor(1, 1)))\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable visualization\n",
    "\n",
    "In this part, we will enable tensorboard visualization in optimizer.\n",
    "The log files will be write to '/tmp/widedeep/${sc.applicationId}', you can use 'tensorboard --logdir /tmp/widedeep' to start the tensorboard to see the Loss, Top1Accuracy and Parameters' distribution during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.intel.analytics.bigdl.optim.DistriOptimizer@4faa530d"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logdir = \"/tmp/widedeep\"\n",
    "val appName = s\"${sc.applicationId}\"\n",
    "val trainSummary = TrainSummary(logdir, appName)\n",
    "trainSummary.setSummaryTrigger(\"Parameters\", Trigger.severalIteration(10))\n",
    "val validationSummary = ValidationSummary(logdir, appName)\n",
    "optimizer.setTrainSummary(trainSummary).setValidationSummary(validationSummary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "After building the model and reading in the dataset, we can train the model using `optimize()` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't find locality partition for partition 0 Partition locations are (ArrayBuffer(localhost)) Candidate partition locations are\n",
      "(0,List()).\n"
     ]
    }
   ],
   "source": [
    "val trainedModel = optimizer.setOptimMethod(optimMethod).setValidation(Trigger.everyEpoch,\n",
    "      validateSet, Array(new Top1Accuracy[Float],\n",
    "        new Loss[Float]()),\n",
    "      batchSize = batchSize, \n",
    "      miniBatch = new SparseTensorMiniBatch[Float](Array(\n",
    "        Tensor.sparse(Array(5006), 1),\n",
    "        Tensor(1, 40)),\n",
    "        Array(Tensor(1, 1)))).setEndWhen(Trigger.maxEpoch(100)).optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "Finally we evaluate the model using the pre-trained model. We set the evaluation information we want to see, like Top1Accuracy and Loss, and set the batch size and miniBatch example.\n",
    "\n",
    "The output is as follows.\n",
    "\n",
    "We can see that the accuracy was improved from about `83.6%` using a wide-only linear model to about `85.1%` using a Wide and Deep model. If you'd like to see a working end-to-end example, you can download our example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1Accuracy is Accuracy(correct: 13815, count: 16281, accuracy: 0.8485351022664456)\n",
      "Loss is (Loss: 108.03325, count: 326, Average Loss: 0.33139032)\n"
     ]
    }
   ],
   "source": [
    "val result = trainedModel.evaluate(validateSet,\n",
    "   Array(new Top1Accuracy[Float], new Loss[Float]), Some(100), miniBatch = new SparseTensorMiniBatch[Float](Array(Tensor.sparse(Array(5006), 1), Tensor(1, 40)), Array(Tensor(1, 1))))\n",
    "\n",
    "result.foreach(r => println(s\"${r._2} is ${r._1}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that this tutorial is just a quick example on a small dataset to get you familiar with the API. Wide and Deep Learning will be even more powerful if you try it on a large dataset with many sparse feature columns that have a large number of possible feature values. Again, feel free to take a look at our research paper for more ideas about how to apply Wide and Deep Learning in real-world large-scale machine learning problems."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
