{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.intel.analytics.bigdl.dataset.SparseTensorMiniBatch\n",
    "import com.intel.analytics.bigdl.nn.{CrossEntropyCriterion, Module}\n",
    "import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "import com.intel.analytics.bigdl.models.widedeep_tutorial.SparseWideDeep\n",
    "import com.intel.analytics.bigdl.optim._\n",
    "import com.intel.analytics.bigdl.tensor.Tensor\n",
    "import com.intel.analytics.bigdl.utils.{Engine, LoggerFilter}\n",
    "import com.intel.analytics.bigdl.dataset.{Sample, TensorSample}\n",
    "import com.intel.analytics.bigdl.tensor.{Storage, Tensor}\n",
    "import com.intel.analytics.bigdl.utils.{File, T}\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkContext\n",
    "import scopt.OptionParser\n",
    "import java.nio.file.Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val conf = Engine.createSparkConf().setAppName(\"Wide and Deep Learning on Census\").set(\"spark.rpc.message.maxSize\", \"1024\").set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "val sc = new SparkContext(conf)\n",
    "Engine.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val AGE = 0\n",
    "val WORKCLASS = 1\n",
    "val FNLWGT = 2\n",
    "val EDUCATION = 3\n",
    "val EDUCATION_NUM = 4\n",
    "val MARITAL_STATUS = 5\n",
    "val OCCUPATION = 6\n",
    "val RELATIONSHIP = 7\n",
    "val RACE = 8\n",
    "val GENDER = 9\n",
    "val CAPITAL_GAIN = 10\n",
    "val CAPITAL_LOSS = 11\n",
    "val HOURS_PER_WEEK = 12\n",
    "val NATIVE_COUNTRY = 13\n",
    "val LABEL = 14\n",
    "\n",
    "val LABEL_COLUMN = \"label\"\n",
    "val CSV_COLUMNS = Array(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "    \"income_bracket\")\n",
    "val CATEGORICAL_COLUMNS = Array(\"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "    \"relationship\", \"race\", \"gender\", \"native_country\")\n",
    "val CONTINUOUS_COLUMNS = Array(\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n",
    "    \"hours_per_week\")\n",
    "val education_vocab = Array(\"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "    \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "    \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "    \"Preschool\", \"12th\") // 16\n",
    "val marital_status_vocab = Array(\"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n",
    "    \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\")\n",
    "    val relationship_vocab = Array(\"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n",
    "    \"Other-relative\")  // 6\n",
    "val workclass_vocab = Array(\"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n",
    "    \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\") // 9\n",
    "val gender_vocab = Array(\"Female\", \"Male\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAgeboundaries(age: String, start: Int = 0): Int = {\n",
    "    if (age == \"?\") 0 + start\n",
    "    else {\n",
    "      val numage = age.toInt\n",
    "      if (numage < 18) 0 else if (numage < 25) 1 else if (numage < 30) 2 else if (numage < 35) 3\n",
    "      else if (numage < 40) 4 else if (numage < 45) 5 else if (numage < 50) 6\n",
    "      else if (numage < 55) 7 else if (numage < 60) 8 else if (numage < 65) 9 else 10\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashbucket(sth: String, bucketsize: Int = 1000, start: Int = 0): Int = {\n",
    "    (sth.hashCode() % bucketsize + bucketsize) % bucketsize + start\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categoricalFromVocabList(sth: String,\n",
    "    vocab_list: Array[String], default: Int = 1, start: Int = 0): Int = {\n",
    "    start + (if (vocab_list.contains(sth)) vocab_list.indexOf(sth) else default)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/**\n",
    " * Load data of Census dataset.\n",
    " *\n",
    " * @param src RDD of file line\n",
    " * @param tag \"Train\" or \"Test\", represents train data or test data\n",
    " * @return\n",
    " */\n",
    "def load(src: RDD[String], tag: String = \"Train\"): RDD[Sample[Float]] = {\n",
    "\n",
    "    val iter = if (tag == \"Train\") src.filter(s => (s.length > 0)).map(_.trim().split(\",\"))\n",
    "    else src.filter(s => (!s.contains(\"|1x3 Cross validator\") && s.length > 0))\n",
    "      .map(_.trim().split(\",\"))\n",
    "\n",
    "    val storage = Storage[Float](11)\n",
    "    val storageArray = storage.array()\n",
    "    val results = iter.map(line => {\n",
    "      val indices = new Array[Int](11)\n",
    "      val lis = line.map(_.trim()).toSeq\n",
    "      for (k <- 0 until 5) indices(k) = k\n",
    "      indices(5) = hashbucket(lis(OCCUPATION), 1000, start = 0) + 5\n",
    "      indices(6) = hashbucket(lis(NATIVE_COUNTRY), 1000, start = 0) + 1005\n",
    "      indices(7) = 2005\n",
    "      indices(8) = hashbucket(lis(EDUCATION) + lis(OCCUPATION), 1000, start = 0) + 2006 // 2006\n",
    "      indices(9) = hashbucket(\n",
    "        getAgeboundaries(lis(AGE)).toString + lis(EDUCATION) + lis(OCCUPATION), 1000) + 3006 // 2006\n",
    "      indices(10) = hashbucket(lis(NATIVE_COUNTRY) + lis(OCCUPATION), 1000) + 4006 // 4006\n",
    "\n",
    "      // 5006\n",
    "      storageArray(0) = categoricalFromVocabList(lis(GENDER), gender_vocab, default = -1, start = 0)\n",
    "      storageArray(1) = categoricalFromVocabList(\n",
    "        lis(EDUCATION), education_vocab, default = -1, start = 0)\n",
    "      storageArray(2) = categoricalFromVocabList(\n",
    "        lis(MARITAL_STATUS), marital_status_vocab, default = -1, start = 0)\n",
    "      storageArray(3) = categoricalFromVocabList(\n",
    "        lis(RELATIONSHIP), relationship_vocab, default = -1, start = 0)\n",
    "      storageArray(4) = categoricalFromVocabList(\n",
    "        lis(WORKCLASS), workclass_vocab, default = -1, start = 0)\n",
    "\n",
    "      storageArray(5) = 1\n",
    "      storageArray(6) = 1\n",
    "      storageArray(7) = getAgeboundaries(lis(AGE), 0)\n",
    "\n",
    "      for (k <- 8 until 11) storageArray(k) = 1\n",
    "\n",
    "      val sps = Tensor.sparse(Array(indices), storage, Array(5006), 11)\n",
    "      val den = Tensor[Float](40).fill(0)\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(WORKCLASS), workclass_vocab, start = 1), 1\n",
    "      ) // 9\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(EDUCATION), education_vocab, start = 10), 1\n",
    "      ) // 16\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(GENDER), gender_vocab, start = 26), 1\n",
    "      ) // 2\n",
    "      den.setValue(\n",
    "        categoricalFromVocabList(lis(RELATIONSHIP), relationship_vocab,\n",
    "          start = 28), 1\n",
    "      ) // 6\n",
    "      // total : 33\n",
    "      den.setValue(34, hashbucket(lis(NATIVE_COUNTRY), 1000, 1).toFloat)\n",
    "      den.setValue(35, hashbucket(lis(OCCUPATION), 1000, 1).toFloat)\n",
    "      den.setValue(36, lis(AGE).toFloat)\n",
    "      den.setValue(37, lis(EDUCATION_NUM).toFloat)\n",
    "      den.setValue(38, lis(CAPITAL_GAIN).toFloat)\n",
    "      den.setValue(39, lis(CAPITAL_LOSS).toFloat)\n",
    "      den.setValue(40, lis(HOURS_PER_WEEK).toFloat)\n",
    "      den.resize(1, 40)\n",
    "      val train_label = if (lis(LABEL).contains(\">50K\")) Tensor[Float](T(2.0f))\n",
    "                        else Tensor[Float](T(1.0f))\n",
    "      train_label.resize(1, 1)\n",
    "\n",
    "      TensorSample[Float](Array(sps, den), Array(train_label))\n",
    "    })\n",
    "    results\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchSize = 600\n",
    "val trainData = \"./census/train.data\"\n",
    "val testData = \"./census/test.data\"\n",
    "val trainDataSet = load(sc.textFile(Paths.get(trainData).toString), \"Train\")\n",
    "val validateSet = load(sc.textFile(Paths.get(testData).toString), \"Test\")\n",
    "\n",
    "val model = SparseWideDeep[Float](modelType = \"wide_n_deep\", classNum = 2)\n",
    "// println(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val optimMethod = new Adam[Float](\n",
    "          learningRate = 0.001,\n",
    "          learningRateDecay = 0.0005\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val optimizer = Optimizer(\n",
    "        model = model,\n",
    "        sampleRDD = trainDataSet,\n",
    "        criterion = new CrossEntropyCriterion[Float](),\n",
    "        batchSize = batchSize,\n",
    "        miniBatch = new SparseTensorMiniBatch[Float](Array(\n",
    "          Tensor.sparse(Array(5006), 1),\n",
    "          Tensor(1, 40)),\n",
    "          Array(Tensor(1, 1)))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't find locality partition for partition 0 Partition locations are (ArrayBuffer(172.168.0.22)) Candidate partition locations are\n",
      "(0,List()).\n"
     ]
    }
   ],
   "source": [
    "val trainedModel = optimizer.setOptimMethod(optimMethod).setValidation(Trigger.everyEpoch,\n",
    "      validateSet, Array(new Top1Accuracy[Float],\n",
    "        new Loss[Float](new CrossEntropyCriterion[Float]())),\n",
    "      batchSize = batchSize,\n",
    "      miniBatch = new SparseTensorMiniBatch[Float](Array(\n",
    "        Tensor.sparse(Array(5006), 1),\n",
    "        Tensor(1, 40)),\n",
    "        Array(Tensor(1, 1)))).setEndWhen(Trigger.maxEpoch(20)).optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2057106\n",
       "-1.7911849\n",
       "1.303036\n",
       "-0.12848523\n",
       "0.66809064\n",
       "0.60296136\n",
       "0.5408805\n",
       "-1.1922419\n",
       "-0.52261984\n",
       "0.97402906\n",
       "-1.0512428\n",
       "-0.35746256\n",
       "-0.28114575\n",
       "-0.19829594\n",
       "-0.5377282\n",
       "0.22858697\n",
       "4.353018\n",
       "-0.6874456\n",
       "0.9123058\n",
       "-3.27013\n",
       "1.8044297\n",
       "-1.741428\n",
       "-1.9338946\n",
       "-0.5336254\n",
       "0.14718446\n",
       "1.6903374\n",
       "-0.1187511\n",
       "-1.2775198\n",
       "2.0769305\n",
       "-0.68084395\n",
       "-0.68264616\n",
       "-0.23208107\n",
       "0.62686116\n",
       "0.2869092\n",
       "1.359752\n",
       "-0.74884737\n",
       "0.3248731\n",
       "-0.16416597\n",
       "2.2178771\n",
       "1.0740796\n",
       "0.38896376\n",
       "-0.10943797\n",
       "-1.4376335\n",
       "-0.97883636\n",
       "0.5645823\n",
       "0.6205644\n",
       "0.7705192\n",
       "0.13986462\n",
       "0.69354033\n",
       "-1.3272737\n",
       "-0.22880793\n",
       "-0.16947827\n",
       "-1.8390166\n",
       "0.86883116\n",
       "-0.2093836\n",
       "-0.5267181\n",
       "1.9036882\n",
       "0.85405916\n",
       "-0.30341175\n",
       "-1.2195041\n",
       "1.1937811\n",
       "-0.9920934\n",
       "-1.0179375\n",
       "0.85659236\n",
       "-1.6623333\n",
       "0.6087423\n",
       "2.2310846\n",
       "-0.75958776\n",
       "-0..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedModel.getParameters()._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 4718.0 failed 1 times, most recent failure: Lost task 1.0 in stage 4718.0 (TID 2368, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1\n",
       "\tat com.intel.analytics.bigdl.dataset.MiniBatch$.resizeData(MiniBatch.scala:299)\n",
       "\tat com.intel.analytics.bigdl.dataset.MiniBatch$.resize(MiniBatch.scala:326)\n",
       "\tat com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:203)\n",
       "\tat com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:110)\n",
       "\tat com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:344)\n",
       "\tat com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:322)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n",
       "\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat com.intel.analytics.bigdl.dataset.MiniBatch$.resizeData(MiniBatch.scala:299)\n",
       "\tat com.intel.analytics.bigdl.dataset.MiniBatch$.resize(MiniBatch.scala:326)\n",
       "\tat com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:203)\n",
       "\tat com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:110)\n",
       "\tat com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:344)\n",
       "\tat com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:322)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "\tat scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n",
       "\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "  at com.intel.analytics.bigdl.optim.Evaluator.test(Evaluator.scala:70)\n",
       "  at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.evaluate(AbstractModule.scala:551)\n",
       "  ... 52 elided\n",
       "Caused by: java.lang.ArrayIndexOutOfBoundsException: 1\n",
       "  at com.intel.analytics.bigdl.dataset.MiniBatch$.resizeData(MiniBatch.scala:299)\n",
       "  at com.intel.analytics.bigdl.dataset.MiniBatch$.resize(MiniBatch.scala:326)\n",
       "  at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:203)\n",
       "  at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:110)\n",
       "  at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:344)\n",
       "  at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:322)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n",
       "  at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)\n",
       "  at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1011)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1009)\n",
       "  at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "  at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1980)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = trainedModel.evaluate(validateSet,\n",
    "   Array(new Top1Accuracy[Float]), Some(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
